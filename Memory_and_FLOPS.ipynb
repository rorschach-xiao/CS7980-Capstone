{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02795142-31e1-4e54-bf9d-794f718fc16c",
   "metadata": {},
   "source": [
    "## Memory usage and FLOPs of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e95a54-154c-400f-b672-9cd9bcc4a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import psutil\n",
    "import subprocess\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from thop import profile, clever_format\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350d49dd-5709-4934-a1d7-8d521cf5e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 0  # Use the first GPU\n",
    "else:\n",
    "    device = -1  # Use the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe7da1-46fa-48fb-8780-8ea186b2442b",
   "metadata": {},
   "source": [
    "## Functions to monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07a490f-15c6-40d5-9fae-a32301714d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current CPU memory usage\n",
    "def get_cpu_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return int(mem_info.rss / (1024 ** 2))  # Convert bytes to MB\n",
    "\n",
    "# Function to get current GPU memory usage\n",
    "def get_gpu_memory_usage():\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    gpu_memory = result.stdout.strip()\n",
    "    return int(gpu_memory)  # Memory in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707023a-6e36-42da-ad57-754611bba9c6",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5769de0e-471a-48c5-bc0a-69e76edbf2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 14 21:09:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              68W / 500W |      8MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e091b210-757c-4e8b-96a1-81604a49927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of distilBERT: 8 MB\n",
      "GPU memory usage after loading distilBERT: 720 MB\n",
      "GPU memory used by distilBERT: 712 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of distilBERT: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_distilbert = pipeline(\"text-classification\", model=\"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading distilBERT: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by distilBERT: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cc35e9-b152-4bc8-abc5-f26b2132cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of distilBERT: 705 MB\n",
      "CPU memory usage after loading distilBERT: 960 MB\n",
      "CPU memory used by distilBERT: 255 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of distilBERT: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_distilbert = pipeline(\"text-classification\", model=\"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading distilBERT: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by distilBERT: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76778b55-b1cf-4b00-bd2f-b0736d5ef891",
   "metadata": {},
   "source": [
    "### ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857d8112-82b8-4e07-b236-2bdbf47c8d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 14 21:09:37 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              76W / 500W |    720MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     53646      C   ...entos7/anaconda3/2022.05/bin/python      706MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c9011a-64d6-4d11-ab3e-f4ca5e82d388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of ELECTRA: 720 MB\n",
      "GPU memory usage after loading ELECTRA: 900 MB\n",
      "GPU memory used by ELECTRA: 180 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of ELECTRA: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_electra = pipeline(\"text-classification\", model=\"pig4431/amazonPolarity_ELECTRA_5E\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading ELECTRA: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by ELECTRA: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a3a580a-5899-44b9-910a-6bc3ad364237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of ELECTRA: 1246 MB\n",
      "CPU memory usage after loading ELECTRA: 1552 MB\n",
      "CPU memory used by ELECTRA: 306 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of ELECTRA: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_electra = pipeline(\"text-classification\", model=\"pig4431/amazonPolarity_ELECTRA_5E\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading ELECTRA: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by ELECTRA: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa684e8b-329c-40c1-b88e-79120e014dc6",
   "metadata": {},
   "source": [
    "### Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00f8240-f7f8-42f9-8d06-162d1752fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 14 21:09:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              76W / 500W |    900MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     53646      C   ...entos7/anaconda3/2022.05/bin/python      886MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6da0366-8558-44ff-85ae-2efc87e32096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of Flan-T5: 900 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2342403a3f2b410299d0dcb20fe1b7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage after loading Flan-T5: 43312 MB\n",
      "GPU memory used by Flan-T5: 42412 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of Flan-T5: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanT5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-xxl\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading Flan-T5: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by Flan-T5: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f40119-828b-45e6-90a0-a6ca66f1dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of Flan-T5: 1557 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8b344ee1a54587b340e8ca35366e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU memory usage after loading Flan-T5: 43975 MB\n",
      "CPU memory used by Flan-T5: 42418 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of Flan-T5: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanT5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-xxl\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading Flan-T5: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by Flan-T5: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617d11-c6d2-4b03-94c0-7af5ad99c89e",
   "metadata": {},
   "source": [
    "### Flan-UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18416211-777e-40be-8013-ad6e49a9d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 14 21:11:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              67W / 500W |  43312MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     53646      C   ...entos7/anaconda3/2022.05/bin/python    43298MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff7a97f3-be7a-489f-be6a-a862a3d2c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of Flan-UL2: 43312 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a8df36549d4d38a9541adebe8124f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage after loading Flan-UL2: 74716 MB\n",
      "GPU memory used by Flan-UL2: 31404 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of Flan-UL2: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanUL2 = pipeline(\"text2text-generation\", model=\"google/flan-ul2\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading Flan-UL2: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by Flan-UL2: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cd4dbcd-3686-4764-b6da-c0f9aa70f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of Flan-UL2: 43980 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991bc844df9248bb9194579dd82ce3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU memory usage after loading Flan-UL2: 118216 MB\n",
      "CPU memory used by Flan-UL2: 74236 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of Flan-UL2: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanUL2 = pipeline(\"text2text-generation\", model=\"google/flan-ul2\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading Flan-UL2: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by Flan-UL2: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac5daf-8745-4f66-b179-74e44c437306",
   "metadata": {},
   "source": [
    "|Model|DistilBERT|ELECTRA|FlanT5|FlanUL2|\n",
    "|-|-|-|-|-|\n",
    "|GPU(MB)|712|180|42412|31404|\n",
    "|CPU(MB)|255|306|42418|74236|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cb2eb-5add-416a-8a6f-81861d792d78",
   "metadata": {},
   "source": [
    "### FLOPs Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725df5e4-d92c-42e1-a2d3-d8e3d0d20114",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "791a82d1-eb5c-4c58-967d-d37d6dfce9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "MACs: 340.649M\n",
      "FLOPs: 681.299M\n",
      "Number of parameters: 43.121M\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c41c0-7193-4af0-999e-c4d87e49bb9d",
   "metadata": {},
   "source": [
    "### ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7032bddb-eef7-482d-ba41-1dc1fbd07ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "MACs: 680.683M\n",
      "FLOPs: 1.361G\n",
      "Number of parameters: 85.648M\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"pig4431/amazonPolarity_ELECTRA_5E\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca20b61-fa85-4a4b-b626-87e9dcd477cf",
   "metadata": {},
   "source": [
    "### Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de325b38-c7d4-4b17-a9de-164c103cdb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebca69972bba4998bda7a3d88ca02ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-xxl and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "MACs: 86.990G\n",
      "FLOPs: 173.980G\n",
      "Number of parameters: 10.888G\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"google/flan-t5-xxl\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03912f89-2485-43a9-9114-818593fe1aaa",
   "metadata": {},
   "source": [
    "### Flan-UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b1aae33-5150-43ef-a75d-1ffd467e8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0b443479574eeea5c03c38f470c6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-ul2 and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "MACs: 154.636G\n",
      "FLOPs: 309.271G\n",
      "Number of parameters: 19.344G\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"google/flan-ul2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70825f9-f3a3-48f9-a004-c32f9c9687a7",
   "metadata": {},
   "source": [
    "|Model|DistilBERT|ELECTRA|FlanT5|FlanUL2|\n",
    "|-|-|-|-|-|\n",
    "|MACs|340.649M|680.683M|86.990G|154.636G|\n",
    "|FLOPs|681.299M|1.361G|173.980G|309.271G|\n",
    "|Params|43.121M|85.648M|10.888G|19.344G|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bf73f-808c-4db4-946a-42eef7d0b4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
