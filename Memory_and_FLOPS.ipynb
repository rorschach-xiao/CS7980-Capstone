{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02795142-31e1-4e54-bf9d-794f718fc16c",
   "metadata": {},
   "source": [
    "## Memory usage and FLOPs of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e95a54-154c-400f-b672-9cd9bcc4a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import psutil\n",
    "import subprocess\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from thop import profile, clever_format\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350d49dd-5709-4934-a1d7-8d521cf5e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 0  # Use the first GPU\n",
    "else:\n",
    "    device = -1  # Use the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe7da1-46fa-48fb-8780-8ea186b2442b",
   "metadata": {},
   "source": [
    "## Functions to monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07a490f-15c6-40d5-9fae-a32301714d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current CPU memory usage\n",
    "def get_cpu_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return int(mem_info.rss / (1024 ** 2))  # Convert bytes to MB\n",
    "\n",
    "# Function to get current GPU memory usage\n",
    "def get_gpu_memory_usage():\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    gpu_memory = result.stdout.strip()\n",
    "    return int(gpu_memory)  # Memory in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707023a-6e36-42da-ad57-754611bba9c6",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5769de0e-471a-48c5-bc0a-69e76edbf2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 20:06:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              66W / 500W |    806MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     48715      C   ...entos7/anaconda3/2022.05/bin/python      792MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e091b210-757c-4e8b-96a1-81604a49927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of distilBERT: 806 MB\n",
      "GPU memory usage after loading distilBERT: 1518 MB\n",
      "GPU memory used by distilBERT: 712 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of distilBERT: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_distilbert = pipeline(\"text-classification\", model=\"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading distilBERT: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by distilBERT: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6cc35e9-b152-4bc8-abc5-f26b2132cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of distilBERT: 505 MB\n",
      "CPU memory usage after loading distilBERT: 777 MB\n",
      "CPU memory used by distilBERT: 272 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of distilBERT: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_distilbert = pipeline(\"text-classification\", model=\"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading distilBERT: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by distilBERT: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76778b55-b1cf-4b00-bd2f-b0736d5ef891",
   "metadata": {},
   "source": [
    "### ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857d8112-82b8-4e07-b236-2bdbf47c8d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 20:09:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              66W / 500W |    806MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     48715      C   ...entos7/anaconda3/2022.05/bin/python      792MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c9011a-64d6-4d11-ab3e-f4ca5e82d388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of ELECTRA: 806 MB\n",
      "GPU memory usage after loading ELECTRA: 1698 MB\n",
      "GPU memory used by ELECTRA: 892 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of ELECTRA: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_electra = pipeline(\"text-classification\", model=\"pig4431/amazonPolarity_ELECTRA_5E\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading ELECTRA: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by ELECTRA: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3a580a-5899-44b9-910a-6bc3ad364237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of ELECTRA: 494 MB\n",
      "CPU memory usage after loading ELECTRA: 930 MB\n",
      "CPU memory used by ELECTRA: 436 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of ELECTRA: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipe_electra = pipeline(\"text-classification\", model=\"pig4431/amazonPolarity_ELECTRA_5E\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading ELECTRA: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by ELECTRA: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa684e8b-329c-40c1-b88e-79120e014dc6",
   "metadata": {},
   "source": [
    "### Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00f8240-f7f8-42f9-8d06-162d1752fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 20:10:23 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              66W / 500W |    806MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     48715      C   ...entos7/anaconda3/2022.05/bin/python      792MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6da0366-8558-44ff-85ae-2efc87e32096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of Flan-T5: 806 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5052f5cb62c94744a99f08ae1c32d733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage after loading Flan-T5: 43704 MB\n",
      "GPU memory used by Flan-T5: 42898 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of Flan-T5: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanT5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-xxl\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading Flan-T5: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by Flan-T5: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f40119-828b-45e6-90a0-a6ca66f1dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of Flan-T5: 1099 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fa286f31734140baff95ce730e6978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU memory usage after loading Flan-T5: 43614 MB\n",
      "CPU memory used by Flan-T5: 42515 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of Flan-T5: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanT5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-xxl\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading Flan-T5: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by Flan-T5: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617d11-c6d2-4b03-94c0-7af5ad99c89e",
   "metadata": {},
   "source": [
    "### Flan-UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18416211-777e-40be-8013-ad6e49a9d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 20:12:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              66W / 500W |    806MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     48715      C   ...entos7/anaconda3/2022.05/bin/python      792MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7a97f3-be7a-489f-be6a-a862a3d2c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage of Flan-UL2: 806 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe2d06c74e7456c93fc4d6cf41b48d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage after loading Flan-UL2: 75460 MB\n",
      "GPU memory used by Flan-UL2: 74654 MB\n"
     ]
    }
   ],
   "source": [
    "# GPU Meomory Usage\n",
    "initial_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"Initial GPU memory usage of Flan-UL2: {initial_gpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanUL2 = pipeline(\"text2text-generation\", model=\"google/flan-ul2\", device=0)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_gpu_memory = get_gpu_memory_usage()\n",
    "print(f\"GPU memory usage after loading Flan-UL2: {after_loading_gpu_memory} MB\")\n",
    "\n",
    "gpu_memory_used_by_model = after_loading_gpu_memory - initial_gpu_memory\n",
    "\n",
    "print(f\"GPU memory used by Flan-UL2: {gpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd4dbcd-3686-4764-b6da-c0f9aa70f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CPU memory usage of Flan-UL2: 674 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb747bbe920248fbb4201859b461c77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU memory usage after loading Flan-UL2: 74945 MB\n",
      "CPU memory used by Flan-UL2: 74271 MB\n"
     ]
    }
   ],
   "source": [
    "# CPU Meomory Usage\n",
    "initial_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"Initial CPU memory usage of Flan-UL2: {initial_cpu_memory} MB\")\n",
    "\n",
    "# Load the model\n",
    "pipeFlanUL2 = pipeline(\"text2text-generation\", model=\"google/flan-ul2\", device=-1)\n",
    "\n",
    "# Memory Usage Calculation\n",
    "after_loading_cpu_memory = get_cpu_memory_usage()\n",
    "print(f\"CPU memory usage after loading Flan-UL2: {after_loading_cpu_memory} MB\")\n",
    "\n",
    "cpu_memory_used_by_model = after_loading_cpu_memory - initial_cpu_memory\n",
    "\n",
    "print(f\"CPU memory used by Flan-UL2: {cpu_memory_used_by_model} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac5daf-8745-4f66-b179-74e44c437306",
   "metadata": {},
   "source": [
    "|Model|DistilBERT|ELECTRA|FlanT5|FlanUL2|\n",
    "|-|-|-|-|-|\n",
    "|GPU(MB)|712|892|42898|74654|\n",
    "|CPU(MB)|272|436|42515|74271|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cb2eb-5add-416a-8a6f-81861d792d78",
   "metadata": {},
   "source": [
    "### FLOPs Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725df5e4-d92c-42e1-a2d3-d8e3d0d20114",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791a82d1-eb5c-4c58-967d-d37d6dfce9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "MACs: 340.649M\n",
      "FLOPs: 681.299M\n",
      "Number of parameters: 43.121M\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c41c0-7193-4af0-999e-c4d87e49bb9d",
   "metadata": {},
   "source": [
    "### ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7032bddb-eef7-482d-ba41-1dc1fbd07ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "MACs: 680.683M\n",
      "FLOPs: 1.361G\n",
      "Number of parameters: 85.648M\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"pig4431/amazonPolarity_ELECTRA_5E\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca20b61-fa85-4a4b-b626-87e9dcd477cf",
   "metadata": {},
   "source": [
    "### Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de325b38-c7d4-4b17-a9de-164c103cdb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabcb45e870646b299e9666f4b6f8e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-xxl and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "MACs: 86.990G\n",
      "FLOPs: 173.980G\n",
      "Number of parameters: 10.888G\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"google/flan-t5-xxl\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03912f89-2485-43a9-9114-818593fe1aaa",
   "metadata": {},
   "source": [
    "### Flan-UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1aae33-5150-43ef-a75d-1ffd467e8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1f763d332c4acf86537b95f5a6ac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-ul2 and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "MACs: 154.636G\n",
      "FLOPs: 309.271G\n",
      "Number of parameters: 19.344G\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"google/flan-ul2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define an example input\n",
    "example_text = \"This is a great product!\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
    "example_input = (inputs['input_ids'],)\n",
    "\n",
    "# Calculate MACs and params\n",
    "macs, params = profile(model, inputs=example_input)\n",
    "flops = macs * 2  # Each MAC operation corresponds to 2 FLOPs (1 multiplication + 1 addition)\n",
    "macs, flops, params = clever_format([macs, flops, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70825f9-f3a3-48f9-a004-c32f9c9687a7",
   "metadata": {},
   "source": [
    "|Model|DistilBERT|ELECTRA|FlanT5|FlanUL2|\n",
    "|-|-|-|-|-|\n",
    "|MACs|340.649M|680.683M|86.990G|154.636G|\n",
    "|FLOPs|681.299M|1.361G|173.980G|309.271G|\n",
    "|Params|43.121M|85.648M|10.888G|19.344G|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bf73f-808c-4db4-946a-42eef7d0b4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
